import math
import random
E = math.e
random.seed(31415)


class Layer:
    def __init__(self, previous_height, height):
        self.biases = [(1*(random.random())*2-1) for n in range(height)]
        self.weights = [[(1*(random.random())*2-1) for n in range(previous_height)] for m in range(height)]
       
        self.delta_biases = [0] * height
        self.delta_weights = [[0] * previous_height for _ in range(height)]
    
    def forward(self, previous_layer_outputs):
        self.previous_layer_outputs = previous_layer_outputs
        self.outputs = [0]*len(self.biases)
        for j in range(len(self.biases)):
            for k in range(len(self.weights[0])):
                self.outputs[j] += ((previous_layer_outputs[k]) * (self.weights[j][k]))
            self.outputs[j] += self.biases[j]

    def activation_function(self, activation_function_type, is_last_layer):
        self.is_last_layer = is_last_layer
        self.activation_function_type = activation_function_type
        if self.activation_function_type == "ReLU":
            if self.is_last_layer == False:
                self.outputs = [max(0, n) for n in self.outputs]
        if self.activation_function_type == "Leaky_ReLU":
            if self.is_last_layer == False:
                self.outputs = [0.1*n if n<0 else n for n in self.outputs]
        if self.activation_function_type == "Softmax":
            maximum_output = max(self.outputs)
            self.outputs = [self.outputs[n]-maximum_output for n in range(len(self.outputs))]
            sum = 0
            for i in range(len(self.outputs)):
                sum += E**(self.outputs[i])
            for i in range(len(self.outputs)):
                self.outputs[i] = (E**(self.outputs[i]))/sum

    def loss(self, prediced_list, expected_list, type):
        self.loss_type = type
        if self.loss_type == "mse":
            self.mean_loss = 0
            for i in range(len(prediced_list)):
                self.mean_loss += 0.5*((prediced_list[i] - expected_list[i])**2)
            self.mean_loss /= len(prediced_list)
            self.d_loss = [(prediced_list[i] - expected_list[i]) for i in range(len(prediced_list))]
        elif self.loss_type == "log":
            epsilon = 1e-15  # Small constant to avoid taking the logarithm of zero
            loss = [math.log(max(prediced_list[i] - expected_list[i], epsilon)) for i in range(len(prediced_list))]
            self.mean_loss = 0
            for i in range(len(loss)):
                self.mean_loss += loss[i]
        
    def back_prop(self, inputted_loss_array):
        self.passed_on_loss_array = inputted_loss_array
        if self.activation_function_type == "ReLU":
            if self.is_last_layer == False:
                for i in range(len(self.passed_on_loss_array)):
                    if self.passed_on_loss_array[i]<1:
                        self.passed_on_loss_array[i] = 0
        if self.activation_function_type == "Leaky_ReLU":
            if self.is_last_layer == False:
                for i in range(len(self.passed_on_loss_array)):
                    if self.passed_on_loss_array[i] < 0:
                        self.passed_on_loss_array[i] *= 0.1
        if self.activation_function_type == "Softmax":
            for i in range(len(self.passed_on_loss_array)):
                #self.passed_on_loss_array[i] *= (1 - self.outputs[i]) * self.outputs[i]
                self.passed_on_loss_array[i] *= E**self.passed_on_loss_array[i]

        for i in range(len(self.biases)):
            self.delta_biases[i] += self.passed_on_loss_array[i]

        for i in range(len(self.weights)):
            for j in range(len(self.weights[0])):
                self.delta_weights[i][j] += self.passed_on_loss_array[i]*self.previous_layer_outputs[j]

        loss_to_pass = [0] * len(self.weights[0])
        for i in range(len(loss_to_pass)):
            for j in range(len(self.weights)):
                loss_to_pass[i] += self.passed_on_loss_array[j] * self.weights[j][i]
        self.loss_to_pass = loss_to_pass

    def update_w_and_b(self, batch_size, learning_rate):
        for i in range(len(self.delta_weights)):
            for j in range(len(self.delta_weights[0])):
                self.delta_weights[i][j] /= batch_size
                self.delta_weights[i][j] *= learning_rate
                self.weights[i][j] -= self.delta_weights[i][j]
        for i in range(len(self.biases)):
            self.delta_biases[i] /= batch_size
            self.delta_biases[i] *= learning_rate
            self.biases[i] -= self.delta_biases[i]

        self.delta_biases = [0] * len(self.biases)
        self.delta_weights = [[0] * len(self.weights[0]) for _ in range(len(self.weights))]


#Sin
classification_data = [[2.21], [37.49], [-1.12], [-23.21], [21.98], [4.65], [-23.26], [-46.4], [-2.05], [33.88], [44.01], [-28.56], [36.36], [19.04], [-42.52], [-38.74], [-48.55], [33.76], [0.32], [19.28], [-4.14], [-45.93], [39.12], [0.65], [-20.38], [-17.62], [39.87], [1.54], [-6.95], [42.92], [20.11], [0.6], [30.74], [30.04], [0.32], [39.6], [23.0], [-18.64], [-9.6], [-2.44], [4.04], [-49.62], [42.17], [-4.6], [-41.81], [16.05], [29.7], [-44.8], [-19.04], [-40.93], [-23.24], [25.62], [29.42], [-23.77], [15.68], [38.38], [13.75], [-16.24], [-39.9], [23.45], [1.17], [-32.89], [34.98], [26.99], [15.77], [26.93], [47.39], [31.54], [6.2], [12.85], [-46.15], [16.55], [35.6], [19.44], [14.45], [-20.64], [25.44], [46.1], [19.94], [17.0], [49.32], [22.5], [22.86], [-19.63], [-49.26], [-10.65], [49.0], [-20.43], [16.96], [-35.31], [39.33], [-15.46], [40.05], [48.2], [-10.59], [-23.05], [-41.38], [-41.74], [0.51], [25.51], [13.1], [5.13], [-22.47], [-31.48], [-40.94], [-6.61], [19.3], [45.19], [8.16], [34.96], [-5.41], [17.12], [16.0], [-26.47], [49.96], [-32.9], [-10.78], [6.18], [13.96], [-22.23], [16.04], [28.81], [-15.93], [-33.27], [-10.02], [-23.32], [-47.57], [-36.89], [-43.7], [-37.49], [18.72], [41.57], [29.85], [-34.98], [-25.08], [-22.03], [-36.58], [16.1], [49.22], [-18.17], [-46.96], [37.39], [-21.78], [8.25], [-39.5], [20.67], [28.85], [13.65], [31.38], [48.33], [25.79], [-44.4], [-7.68], [-3.87], [13.78], [-18.03], [-19.58], [-6.44], [-13.61], [-23.24], [-39.66], [-16.17], [22.66], [-24.48], [-42.75], [6.42], [-16.5], [-37.95], [-2.14], [43.73], [26.07], [-4.38], [30.45], [9.83], [10.42], [10.28], [17.99], [-41.8], [7.15], [-48.09], [-36.64], [44.13], [-10.42], [41.23], [-26.11], [46.96], [22.55], [15.24], [-15.77], [-49.46], [21.84], [-30.96], [10.16], [-6.99], [-23.84], [-14.77], [-31.84], [24.7], [-20.43], [-25.4], [-11.69], [28.65], [36.61], [36.87], [41.92], [-5.29], [31.55], [-7.48], [-26.49], [-37.53], [-7.74], [-11.53], [47.48], [-44.02], [-33.03], [-17.04], [9.96], [25.96], [-17.16], [-35.03], [37.73], [18.44], [8.49], [-44.38], [7.84], [-47.6], [37.09], [19.38], [-2.29], [-20.21], [-42.69], [-44.79], [22.91], [8.74], [10.49], [-40.83], [30.1], [-46.26], [49.92], [13.36], [-28.08], [-21.68], [-36.41], [37.26], [43.99], [13.89], [32.7], [-46.74], [-33.08], [-16.31], [4.04], [45.37], [35.98], [5.69], [37.27], [43.23], [-36.97], [33.68], [34.75], [-4.23], [27.8], [17.56], [13.72], [0.15], [48.28], [-36.44], [-35.99], [1.99], [-48.92], [-12.42], [-22.82], [-11.37], [-44.99], [-20.39], [43.17], [-37.33], [-40.71], [-28.17], [-40.17], [-25.81], [49.68], [-7.74], [30.27], [-17.24], [3.67], [30.96], [-17.74], [-35.31], [6.12], [-11.44], [-19.55], [11.66], [-40.74], [-7.87], [1.94], [-30.33], [37.16], [27.68], [-21.79], [14.4], [-16.94], [-30.98], [0.75], [39.03], [11.31], [-24.59], [-33.06], [11.55], [37.52], [-48.51], [44.7], [23.06], [-9.37], [28.54], [2.1], [40.52], [-6.56], [26.56], [-15.92], [26.98], [29.58], [-17.48], [23.56], [28.3], [41.3], [-32.97], [-27.18], [-18.44], [-48.19], [48.06], [20.41], [-9.69], [-2.45], [24.34], [41.17], [-48.3], [20.36], [-42.16], [-11.02], [-4.67], [9.76], [27.71], [35.91], [5.45], [-9.62], [-8.6], [-9.69], [28.26], [-45.09], [-48.01], [-14.26], [4.45], [-38.31], [-8.78], [17.89], [-25.0], [39.59], [35.33], [14.09], [27.12], [14.64], [-43.83], [-0.43], [2.78], [-45.68], [-20.4], [33.71], [33.85], [-8.56], [13.69], [-8.72], [-21.7], [-8.79], [21.01], [-33.36], [38.77], [-19.65], [-4.78], [-9.25], [-15.92], [-0.17], [38.36], [12.34], [-21.78], [36.04], [11.0], [-36.92], [-41.27], [-32.18], [-26.23], [-27.75], [7.78], [-36.51], [-32.08], [5.57], [-17.18], [30.35], [9.64], [-17.64], [-9.45], [-22.15], [-30.8], [10.78], [-9.05], [27.42], [-21.68], [27.41], [-3.81], [-16.49], [17.12], [-25.92], [16.88], [36.99], [19.51], [33.21], [44.62], [39.83], [-12.12], [15.45], [-22.58], [-0.74], [13.33], [9.98], [-28.14], [10.43], [34.64], [13.01], [-24.15], [-26.57], [-35.28], [4.66], [33.68], [-20.7], [-46.34], [19.24], [-41.77], [-18.87], [5.56], [30.08], [1.85], [37.24], [41.33], [8.35], [-8.28], [-17.22], [23.83], [11.22], [-22.37], [18.04], [-37.26], [46.86], [37.96], [24.07], [-37.76], [12.45], [18.0], [20.77], [-17.99], [-10.83], [36.52], [-22.26], [-39.81], [25.89], [-15.58], [-0.69], [21.83], [-21.36], [-24.13], [33.52], [19.63], [41.67], [38.33], [22.29], [-24.89], [34.12], [-44.87], [8.7], [7.8], [-9.47], [35.36], [-30.86], [-33.63], [13.66], [0.93], [-11.67], [-38.52], [-7.84], [12.97], [32.89], [9.5], [-40.44], [-48.62], [39.18], [-8.83], [-43.22], [-29.09], [-49.59], [33.78], [3.41], [9.76], [-16.82], [-36.88], [40.3], [-35.44], [-43.25], [-46.16], [8.8], [40.44], [-42.25], [20.11], [47.12], [-15.2], [-14.66], [29.05], [-10.56], [-16.93], [-18.01], [-18.6], [-23.65], [7.97], [40.92], [-41.96], [16.62], [-46.76], [23.38], [-23.52], [45.23], [-49.09], [43.96], [-11.47], [-37.82], [-27.99], [-17.43], [-38.1], [-0.94], [-10.55], [-46.07], [39.15], [-24.86], [-7.33], [5.09], [-32.7], [-35.19], [-43.48], [-42.86], [-8.54], [49.34], [-15.33], [48.63], [27.82], [-18.25], [-47.68], [-3.79], [0.67], [-10.74], [41.0], [40.01], [-38.09], [7.41], [-44.47], [18.2], [3.38], [2.08], [-35.15], [44.7], [-38.49], [-30.65], [35.27], [16.95], [18.33], [-1.82], [-16.56], [47.64], [-14.54], [-17.39], [-17.88], [-22.16], [-37.46], [-30.07], [-42.93], [-4.66], [-36.87], [-30.61], [-35.89], [23.57], [-16.11], [26.15], [47.48], [-38.51], [35.73], [-30.02], [48.42], [25.69], [-10.94], [36.05], [-33.52], [20.06], [-48.29], [-34.91], [-26.13], [-19.77], [-31.99], [-47.23], [-38.43], [25.09], [-23.7], [-29.62], [-43.55], [47.77], [-19.36], [-40.37], [42.65], [38.31], [48.6], [20.26], [38.66], [-38.72], [-38.56], [-41.73], [-41.47], [-17.96], [-6.98], [3.82], [8.65], [-7.81], [-21.78], [-0.33], [-48.94], [-21.97], [-10.09], [29.57], [13.34], [-9.45], [-5.49], [25.89], [37.91], [22.13], [-43.13], [-42.61], [-8.95], [13.8], [22.06], [23.57], [-27.0], [11.88], [46.5], [1.94], [-16.89], [-21.32], [-29.25], [48.65], [19.91], [20.53], [-15.03], [-35.3], [2.36], [14.52], [29.9], [-22.5], [-3.95], [-47.4], [-49.05], [45.77], [30.39], [-42.28], [-23.24], [35.04], [25.17], [10.44], [35.57], [32.22], [-18.73], [-48.03], [36.81], [-42.69], [44.65], [15.18], [11.17], [18.32], [-24.87], [23.43], [8.57], [-0.19], [30.11], [34.75], [39.67], [-34.91], [-30.97], [21.97], [-40.26], [24.02], [10.53], [9.17], [23.24], [-49.93], [-49.72], [1.98], [-34.84], [-26.82], [-35.25], [-40.63], [2.55], [-27.89], [-46.27], [4.96], [-5.05], [35.79], [-15.52], [-9.61], [-45.91], [-38.33], [42.82], [39.23], [-38.94], [-29.76], [37.66], [-4.06], [-6.58], [36.57], [-42.48], [-35.41], [-46.8], [40.3], [40.31], [42.39], [26.96], [27.43], [-44.03], [-35.34], [7.7], [46.99], [39.38], [7.08], [15.78], [-35.42], [-25.36], [17.72], [-41.25], [-8.29], [-49.86], [7.36], [-44.65], [20.42], [-8.0], [-8.81], [25.37], [-23.21], [-41.51], [-14.34], [49.27], [12.8], [5.65], [-49.86], [-16.66], [0.16], [-25.61], [-44.76], [-30.74], [44.11], [38.81], [21.28], [-26.69], [-0.08], [5.91], [41.03], [8.39], [-41.64], [-35.37], [11.75], [4.93], [-47.93], [-20.66], [47.3], [28.81], [-3.74], [37.21], [-24.31], [44.71], [-0.13], [-21.97], [20.27], [-37.91], [8.86], [-23.28], [24.31], [6.46], [30.68], [-38.1], [-6.21], [40.32], [17.31], [20.88], [46.47], [5.25], [-16.78], [-29.41], [-22.24], [-45.51], [38.13], [42.58], [43.99], [48.66], [48.54], [-30.62], [37.35], [-42.69], [11.51], [37.99], [-32.92], [-24.49], [-0.48], [5.99], [8.31], [7.54], [26.29], [12.53], [35.65], [-21.4], [13.74], [43.38], [-47.2], [36.98], [-30.91], [-5.27], [-40.56], [34.41], [0.79], [-3.85], [44.32], [5.52], [49.44], [-4.23], [34.91], [-38.46], [40.7], [-46.15], [12.76], [-5.85], [4.61], [-19.52], [46.81], [-26.39], [26.37], [1.6], [14.85], [-29.92], [-46.42], [-48.28], [35.14], [19.41], [-39.79], [-10.14], [36.67], [25.92], [-29.5], [-0.44], [-33.11], [-13.55], [-17.77], [1.4], [30.09], [2.66], [-33.3], [-42.28], [-19.12], [49.87], [-21.07], [-8.6], [30.09], [31.5], [38.35], [2.02], [28.98], [25.96], [32.96], [-33.83], [-28.9], [-1.61], [-24.35], [0.87], [-19.13], [-4.68], [-19.47], [43.34], [19.49], [-31.07], [2.98], [-23.78], [-22.37], [13.77], [-10.47], [-5.41], [44.31], [25.8], [17.83], [34.18], [-11.1], [-12.1], [-15.65], [-17.77], [10.27], [-21.99], [-46.89], [3.66], [-0.52], [-43.42], [36.04], [36.62], [28.63], [36.74], [12.03], [-0.02], [-45.01], [43.54], [2.08], [17.8], [-21.0], [-13.88], [-40.98], [11.11], [-27.11], [48.53], [-38.04], [-46.4], [-38.99], [-45.32], [0.79], [19.82], [-18.54], [-15.73], [11.78], [38.21], [-40.46], [6.31], [3.9], [45.26], [24.7], [-47.46], [22.72], [16.09], [47.49], [14.1], [-29.18], [9.11], [-17.5], [-42.44], [30.12], [23.77], [-10.59], [-43.08], [6.67], [-17.01], [37.89], [-5.37], [14.77], [6.29], [-10.73], [37.05], [45.91], [-4.39], [-32.91], [-36.86], [-27.39], [-18.44], [-16.4], [23.46], [-24.24], [-43.85], [-40.31], [20.21], [-18.83], [-32.23], [-10.42], [8.73], [-18.07], [-17.49], [-29.78], [-49.28], [-37.8], [-14.07], [-28.82], [43.89], [-21.25], [8.96], [1.04], [-14.99], [17.94], [-4.56], [9.69], [47.72], [25.48], [41.98], [25.64], [-6.06], [-9.93], [0.24], [-13.99], [21.22], [20.02], [34.04]]
classification_answers =[[0.8], [-0.21], [-0.9], [0.94], [0.01], [-1.0], [0.95], [-0.66], [-0.89], [0.63], [0.03], [0.28], [-0.97], [0.19], [0.99], [-0.86], [0.99], [0.72], [0.31], [0.42], [0.84], [-0.93], [0.99], [0.61], [-1.0], [0.94], [0.83], [1.0], [-0.62], [-0.87], [0.95], [0.56], [-0.63], [-0.98], [0.31], [0.95], [-0.85], [0.21], [0.17], [-0.65], [-0.78], [0.6], [-0.97], [0.99], [0.82], [-0.34], [-0.99], [-0.73], [-0.19], [0.09], [0.95], [0.47], [-0.91], [0.98], [0.03], [0.63], [0.93], [0.51], [-0.81], [-0.99], [0.92], [-1.0], [-0.41], [0.96], [-0.06], [0.97], [-0.26], [0.12], [-0.08], [0.28], [-0.83], [-0.75], [-0.86], [0.56], [0.95], [-0.98], [0.3], [0.85], [0.89], [-0.96], [-0.81], [-0.49], [-0.76], [-0.7], [0.84], [0.94], [-0.95], [-1.0], [-0.95], [0.68], [1.0], [-0.25], [0.71], [-0.88], [0.92], [0.87], [0.51], [0.78], [0.49], [0.37], [0.51], [-0.91], [0.46], [-0.06], [0.1], [-0.32], [0.44], [0.93], [0.95], [-0.39], [0.77], [-0.99], [-0.29], [-0.97], [-0.3], [-1.0], [0.98], [-0.1], [0.98], [0.24], [-0.33], [-0.51], [0.22], [-0.96], [0.56], [0.97], [0.43], [0.72], [0.28], [0.21], [-0.13], [-0.67], [-1.0], [0.41], [0.05], [0.04], [0.9], [-0.38], [-0.87], [0.63], [-0.16], [-0.3], [-0.21], [0.92], [-0.97], [0.97], [-0.54], [0.88], [-0.04], [-0.93], [0.61], [-0.41], [-0.98], [0.67], [0.94], [0.73], [-0.67], [-0.16], [-0.86], [0.95], [-0.92], [0.45], [-0.62], [0.61], [0.94], [0.14], [0.71], [-0.25], [-0.84], [-0.25], [0.81], [0.95], [-0.82], [-0.39], [-0.84], [-0.75], [-0.76], [0.82], [0.76], [0.82], [0.87], [0.15], [0.84], [-0.38], [-0.83], [0.16], [-0.53], [0.45], [0.06], [0.72], [0.15], [0.44], [-0.67], [-0.65], [0.96], [-0.81], [-0.41], [-0.42], [-1.0], [-0.26], [0.77], [-0.37], [-0.89], [-0.74], [-0.88], [0.84], [0.13], [-0.93], [-0.98], [0.17], [-0.99], [0.86], [-0.35], [-0.04], [-1.0], [0.97], [-0.51], [0.74], [0.99], [0.46], [0.03], [-0.4], [0.8], [-0.39], [1.0], [0.46], [-0.57], [0.51], [-0.75], [-0.98], [0.96], [-0.72], [-0.79], [0.63], [-0.87], [-0.01], [-0.97], [-0.76], [-0.34], [0.71], [-0.19], [-0.31], [0.96], [-0.43], [0.01], [0.97], [0.96], [-0.37], [-1.0], [0.57], [-0.78], [0.98], [-0.99], [-0.56], [-0.42], [-0.68], [0.67], [0.77], [-0.19], [0.89], [0.46], [-0.96], [0.91], [0.15], [-0.92], [0.95], [0.99], [0.91], [0.97], [0.15], [0.74], [0.93], [-0.85], [-1.0], [-0.73], [0.36], [-0.13], [-0.1], [-0.62], [-0.63], [-0.55], [-0.99], [-0.91], [1.0], [-0.5], [-0.44], [0.9], [0.68], [-0.16], [0.9], [-0.64], [-0.79], [-0.1], [-1.0], [0.93], [0.88], [-0.51], [0.56], [-0.2], [0.97], [0.94], [0.42], [0.68], [0.97], [-0.95], [0.52], [-1.0], [-0.85], [-0.18], [0.98], [0.66], [-0.88], [-0.05], [-0.26], [0.86], [0.32], [-0.27], [0.99], [0.21], [0.96], [-0.97], [0.98], [-1.0], [-0.03], [-0.44], [-1.0], [-0.89], [0.4], [0.88], [-0.81], [1.0], [0.26], [-0.64], [-0.71], [-0.32], [0.92], [1.0], [0.97], [1.0], [1.0], [-0.33], [0.53], [-0.98], [-0.74], [0.19], [-0.73], [0.26], [0.01], [-0.89], [0.77], [-0.99], [-0.97], [-0.57], [-0.6], [-0.82], [0.13], [0.95], [-0.7], [1.0], [0.91], [0.88], [0.15], [-0.42], [0.35], [-0.99], [-1.0], [0.75], [0.65], [-0.76], [0.9], [-0.65], [-0.29], [-0.59], [0.83], [-0.93], [0.88], [-0.72], [1.0], [-0.17], [0.21], [-0.17], [0.61], [-0.22], [-0.21], [-1.0], [-1.0], [0.7], [0.42], [-0.69], [-0.89], [-0.5], [1.0], [0.93], [-0.62], [-0.65], [1.0], [-0.88], [-0.21], [0.94], [0.03], [0.16], [0.58], [-0.98], [-0.37], [0.75], [-0.31], [0.76], [0.62], [0.7], [-0.99], [-0.71], [-0.92], [-0.65], [0.61], [0.98], [0.6], [0.85], [0.43], [0.26], [0.56], [-0.67], [0.69], [-0.53], [-0.13], [-0.84], [-0.08], [0.43], [0.83], [-0.99], [0.66], [-1.0], [0.77], [-0.96], [-0.71], [0.38], [0.8], [-0.02], [-0.66], [-0.97], [0.96], [-0.44], [-0.47], [0.88], [-0.91], [1.0], [-0.96], [-0.97], [0.37], [-0.72], [0.43], [0.26], [0.26], [-0.87], [-0.06], [-0.12], [-0.75], [0.94], [0.76], [0.99], [-0.92], [0.27], [-0.86], [0.69], [-0.13], [-0.64], [0.16], [-0.59], [0.84], [0.86], [0.7], [-0.74], [0.59], [-0.29], [0.24], [0.42], [-0.78], [0.66], [1.0], [0.05], [-0.72], [0.53], [-0.8], [0.89], [0.8], [0.78], [-0.73], [-1.0], [0.39], [1.0], [-0.08], [-0.39], [1.0], [1.0], [-0.56], [0.69], [0.73], [0.63], [0.7], [-0.27], [-0.33], [0.9], [0.73], [0.51], [0.77], [0.67], [-0.82], [0.58], [0.39], [0.99], [0.95], [0.0], [-0.49], [-0.87], [-0.7], [0.91], [0.94], [0.74], [0.25], [1.0], [0.99], [-0.08], [0.9], [-0.79], [-0.36], [-0.98], [1.0], [0.95], [0.92], [-0.02], [0.89], [-0.12], [-0.28], [0.99], [-0.39], [-0.81], [0.9], [-0.87], [0.99], [0.27], [-0.87], [-0.93], [-0.96], [0.59], [0.48], [0.9], [-0.77], [-0.8], [-0.37], [-1.0], [0.44], [0.56], [0.53], [0.6], [0.62], [0.97], [-0.16], [0.74], [-0.38], [0.9], [-0.47], [-0.6], [-0.24], [0.87], [0.56], [0.66], [-0.71], [0.69], [-0.65], [-0.95], [-0.5], [-0.97], [0.75], [-0.49], [-0.92], [0.99], [0.82], [0.17], [0.24], [0.97], [0.87], [1.0], [0.74], [0.72], [0.97], [-1.0], [0.39], [0.85], [-0.35], [-0.72], [-0.92], [0.98], [-0.96], [0.53], [1.0], [-1.0], [-0.86], [0.94], [0.92], [0.35], [-0.84], [-0.8], [-0.54], [0.11], [-0.67], [-0.04], [0.99], [0.97], [0.42], [-0.6], [-0.49], [-0.45], [-0.97], [0.57], [-1.0], [0.99], [0.82], [-0.85], [-0.76], [0.78], [0.59], [0.78], [-0.64], [-0.63], [0.7], [-1.0], [-0.21], [-0.32], [0.97], [-0.02], [0.62], [-0.96], [0.7], [0.03], [0.71], [0.69], [0.21], [-0.14], [0.75], [0.98], [-0.46], [0.94], [-0.07], [-1.0], [-0.96], [-0.63], [0.58], [0.93], [0.93], [-0.62], [0.83], [-1.0], [0.87], [0.99], [-0.63], [0.68], [0.7], [0.93], [-1.0], [0.49], [0.72], [0.27], [0.94], [0.98], [-0.86], [0.99], [0.95], [-0.46], [0.04], [-0.85], [-0.85], [0.72], [0.12], [0.79], [-0.78], [0.96], [0.62], [0.5], [-0.98], [-0.51], [0.26], [-0.99], [0.75], [-0.19], [-0.97], [-0.19], [0.92], [0.35], [0.43], [0.02], [-0.55], [-0.9], [-0.89], [0.25], [-0.95], [0.33], [0.52], [0.92], [0.28], [-0.99], [0.64], [-0.21], [0.56], [-0.37], [-0.75], [-0.97], [0.94], [-0.94], [-0.19], [0.18], [-0.94], [-0.59], [-0.92], [1.0], [-0.95], [1.0], [-0.04], [0.79], [-0.29], [-0.9], [1.0], [0.75], [-0.32], [0.51], [0.51], [-1.0], [0.97], [0.75], [-0.05], [0.71], [0.99], [0.13], [0.99], [0.72], [-0.07], [0.76], [-0.23], [-0.9], [0.4], [-0.91], [0.39], [0.88], [-0.62], [1.0], [-0.99], [-0.58], [0.24], [0.94], [0.62], [-0.98], [-0.84], [0.23], [-0.59], [0.39], [0.81], [0.16], [-0.46], [-0.7], [0.63], [0.13], [0.9], [0.65], [-1.0], [-0.08], [-0.36], [-0.19], [0.86], [0.72], [0.73], [-0.73], [-0.98], [0.72], [-0.97], [-0.18], [-0.51], [0.56], [-0.47], [0.73], [0.67], [-0.13], [-0.02], [0.99], [-0.21], [0.54], [0.96], [-0.73], [0.18], [-0.67], [-0.39], [0.07], [0.5], [-1.0], [0.9], [0.61], [-0.86], [0.88], [0.91], [0.25], [-1.0], [0.42], [-0.99], [0.01], [-1.0], [-0.99], [0.71], [-0.34], [0.96], [-0.87], [0.29], [-1.0], [0.6], [-0.46], [-0.29], [0.9], [0.95], [0.92], [-0.04], [-0.89], [-0.56], [0.92], [-0.57], [0.08], [-0.66], [0.48], [0.85], [-0.28], [0.15], [0.71], [0.65], [0.33], [-0.69], [-0.73], [0.89], [-0.35], [-0.69], [0.14], [-0.83], [0.19], [0.42], [-0.99], [-0.62], [0.31], [-0.95], [0.94], [1.0], [0.76], [1.0], [-0.65], [0.92], [-0.55], [0.53], [-0.87], [0.66], [-0.86], [0.71], [0.94], [-0.43], [-0.99], [-0.83], [0.88], [0.99], [-0.97], [0.46], [-0.95], [0.99], [-0.27], [-0.39], [-0.8], [-0.73], [-0.97], [0.08], [0.61], [0.9], [-0.65], [0.74], [1.0], [-0.67], [0.59], [-1.0], [0.71], [0.76], [-0.28], [1.0], [-0.58], [-0.6], [0.6], [0.34], [0.16], [0.98], [0.37], [0.93], [0.87], [0.77], [0.32], [0.62], [-0.85], [0.37], [0.99], [0.45], [-0.06], [0.88], [-0.75], [-0.0], [-0.23], [-0.5], [-0.5], [0.53], [-1.0], [-0.88], [-0.35], [-0.82], [-0.51], [-0.02], [-0.86], [-0.43], [0.87], [-0.87], [-0.84], [-0.97], [0.14], [-0.99], [-0.92], [-0.99], [-0.33], [-0.66], [-0.96], [-0.97], [0.71], [0.83], [0.3], [0.02], [-0.71], [0.49], [-0.37], [0.03], [-0.69], [0.96], [-0.42], [0.33], [-0.67], [-0.37], [-0.36], [1.0], [0.79], [0.31], [0.98], [1.0], [-0.96], [-0.98], [0.92], [0.78], [0.38], [0.96], [0.19], [0.79], [0.81], [0.01], [0.96], [-0.6], [0.94], [0.95], [-1.0], [0.74], [-0.77], [0.4], [0.64], [-0.99], [0.78], [0.13], [-0.51], [0.98], [0.02], [-0.73], [0.84], [0.64], [0.7], [0.98], [1.0], [0.83], [-0.1], [-1.0], [0.52], [-0.09], [-0.68], [0.45], [0.86], [-0.66], [-0.79], [0.99], [-0.26], [-0.56], [0.34], [-0.91], [0.49], [0.22], [0.48], [0.24], [-0.99], [0.7], [0.92], [0.49]]

testing_data =[[30.6], [-24.14], [23.2], [23.31], [10.09], [17.39], [-19.87], [-37.04], [34.95], [-27.18], [-13.25], [35.69], [-12.12], [49.36], [49.89], [-10.64], [37.86], [-2.37], [-41.08], [-4.71], [-17.51], [-19.85], [29.97], [-8.68], [-9.48], [-45.89], [-40.14], [-7.21], [-37.07], [-11.2], [-16.82], [-30.31], [35.63], [17.73], [-48.73], [28.16], [15.95], [-45.06], [34.06], [-48.89], [-10.32], [-31.18], [26.8], [-49.1], [-44.23], [-27.78], [-6.5], [15.29], [35.31], [49.26]]
testing_answers = [[-0.73], [0.84], [-0.94], [-0.97], [-0.62], [-0.99], [-0.85], [0.61], [-0.38], [-0.89], [-0.63], [-0.91], [0.43], [-0.79], [-0.37], [0.94], [0.16], [-0.7], [0.24], [1.0], [0.97], [-0.84], [-0.99], [-0.68], [0.06], [-0.94], [-0.64], [-0.8], [0.59], [0.98], [0.9], [0.89], [-0.88], [-0.9], [1.0], [0.11], [-0.24], [-0.88], [0.48], [0.98], [0.78], [0.23], [1.0], [0.92], [-0.25], [-0.47], [-0.22], [0.41], [-0.68], [-0.84]]

class NN:
    def __init__(self, input_size, inner_layers_number, height, output_size, inner_layer_activation, last_layer_activation):
        self.inner_layer_activation = inner_layer_activation
        self.last_layer_activation = last_layer_activation
        self.layers = [[]] * (inner_layers_number + 2) 
        self.layers[0] = Layer(input_size, height)
        self.layers[-1] = Layer(height, output_size)
        for i in range(inner_layers_number):
            self.layers[i+1] = Layer(height, height)
    
    def train(self, epochs, learning_rate, data_train, data_output, is_testing, batch_size):
        current_epoch = 0
        current_batch = 0
        for i in range(epochs):
            current_epoch_loss = 0
            for j in range(len(data_train)):
                self.layers[0].forward(data_train[j])
                self.layers[0].activation_function(self.inner_layer_activation, False)
                for k in range(len(self.layers)-2):
                    self.layers[k+1].forward(self.layers[k].outputs)
                    self.layers[k+1].activation_function(self.inner_layer_activation, False)
                self.layers[-1].forward(self.layers[-2].outputs)
                self.layers[-1].activation_function(self.last_layer_activation, True)
                # print(f"Outputted: {self.layers[-1].outputs}")
                # print(f"Actual: {data_output[j]}")
                #now for loss
                if is_testing == False:
                    self.layers[-1].loss(self.layers[-1].outputs, data_output[j],"mse")
                    print(f"Loss: {self.layers[-1].mean_loss}")
                    #now for back prop
                    self.layers[-1].back_prop(self.layers[-1].d_loss)
                    for l in range(len(self.layers)-1):
                        self.layers[-l-2].back_prop(self.layers[-l-1].loss_to_pass)
                    current_batch += 1
                    if current_batch == batch_size:
                        current_batch = 0
                        for i in range(len(self.layers)):
                            self.layers[i].update_w_and_b(batch_size, learning_rate)
                elif is_testing == True:
                    print("----- TESTING -----")
                    if self.last_layer_activation != "Softmax":
                        self.layers[-1].loss(self.layers[-1].outputs, data_output[j],"mse")
                        current_epoch_loss += self.layers[-1].mean_loss
                    elif self.last_layer_activation == "Softmax":
                        #fix to use max value instead of rounded
                        print(self.layers[-1].outputs)
                        predicted_ans = [0] * len(self.layers[-1].outputs)
                        index = predicted_ans.index(max(predicted_ans))
                        for i in range(len(predicted_ans)):
                            if i != index:
                                predicted_ans[i] = 0
                            else:
                                predicted_ans[i] = 1
                        # print(f"predicted: {predicted_ans}")
                        # print(f"actual: {data_output[j]}")
                        if predicted_ans == data_output[j]:
                            pass #fix this
            current_epoch += 1
            print(f"Epochs completed: {current_epoch}/{epochs}\nAverage epoch loss: {current_epoch_loss/len(data_train)}")

    def test(self, testing_data, testing_answers):
        self.train(1, 0, testing_data, testing_answers, True, 1000000000000000)


neural = NN(3, 3, 16, 1, "ReLU", "Leaky_ReLU")
neural.train(1000, 0.001, classification_data, classification_answers, False, 32)
neural.test(testing_data, testing_answers)